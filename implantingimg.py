# -*- coding: utf-8 -*-
"""implantingIMG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vc2u4zi6UPiWJV44cBT-hUG6hqGk6XSI
"""

!pip install torch torchvision matplotlib

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import numpy as np
from torchvision.datasets import CelebA
import random

class CelebADataset(Dataset):
    def __init__(self, root_dir, split="train", transform=None):
        self.dataset = CelebA(root=root_dir, split=split, download=True, transform=transform)
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        img = self.dataset[idx][0]
        img = self.transform(img)

        # Create random mask
        mask = self.create_random_mask(img.size())
        masked_img = img * mask  # Apply mask to the image

        return masked_img, img, mask

    def create_random_mask(self, size):
        """Create a random mask that blocks a part of the image."""
        mask = torch.ones(size)
        x = random.randint(0, size[1] // 2)
        y = random.randint(0, size[2] // 2)
        w = random.randint(size[1] // 4, size[1] // 2)
        h = random.randint(size[2] // 4, size[2] // 2)

        # Zero out part of the mask
        mask[:, x:x+w, y:y+h] = 0
        return mask

import torch
from torch.utils.data import Dataset
import random
from torchvision.datasets import CelebA
from torchvision import transforms

class CelebADataset(Dataset):
    def __init__(self, root_dir, split="train", transform=None):
        # Load the CelebA dataset
        self.dataset = CelebA(root=root_dir, split=split, download=True, transform=transform)
        self.transform = transform  # Store the transform for later use

    def __len__(self):
        return len(self.dataset)  # Return the size of the dataset

    def __getitem__(self, idx):
        # Fetch the image at the given index
        img = self.dataset[idx][0]

        # Apply the transformation (if any) to convert the image into a tensor
        if self.transform:
            img = self.transform(img)

        # Create a random mask of the same size as the image
        mask = self.create_random_mask(img.size())

        # Apply the mask to the image by element-wise multiplication
        masked_img = img * mask

        return masked_img, img, mask

    def create_random_mask(self, size):
        """Create a random mask that blocks a part of the image."""
        # Initialize a mask with all ones (same size as the image)
        mask = torch.ones(size)

        # Randomly generate the top-left corner and dimensions of the mask's blocked area
        x = random.randint(0, size[1] // 2)
        y = random.randint(0, size[2] // 2)
        w = random.randint(size[1] // 4, size[1] // 2)
        h = random.randint(size[2] // 4, size[2] // 2)

        # Zero out the selected region in the mask
        mask[:, x:x+w, y:y+h] = 0
        return mask


# Example of using the CelebADataset with transformations
transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize images to 128x128
    transforms.ToTensor(),          # Convert the image to a tensor
])

dataset = CelebADataset(root_dir='path_to_celeba_dataset', transform=transform)

class ContextEncoder(nn.Module):
    def __init__(self):
        super(ContextEncoder, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()  # Output is an image
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize the images
    transforms.ToTensor(),          # Convert images to PyTorch tensors
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images
])

# Load the dataset
train_dataset = CelebADataset(root_dir='./data', split="train", transform=transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

def train_model(model, dataloader, criterion, optimizer, num_epochs=10):
    model.train()  # Set the model to training mode

    for epoch in range(num_epochs):
        running_loss = 0.0

        for i, (masked_imgs, original_imgs, masks) in enumerate(dataloader):
            # Move data to GPU if available
            masked_imgs, original_imgs, masks = masked_imgs.cuda(), original_imgs.cuda(), masks.cuda()

            optimizer.zero_grad()

            # Forward pass
            outputs = model(masked_imgs)

            # Compute loss only in the masked region
            loss = criterion(outputs * masks, original_imgs * masks)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        # Print loss every epoch
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')

    print('Training finished.')

from PIL import Image
import torch
from torch.utils.data import Dataset
import random
from torchvision import transforms
from torchvision.datasets import CelebA

class CelebADataset(Dataset):
    def __init__(self, root_dir, split="train", transform=None):
        # Load the CelebA dataset
        self.dataset = CelebA(root=root_dir, split=split, download=True, transform=transform)
        self.transform = transform  # Store the transform for later use

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        # Fetch the image at the given index
        img = self.dataset[idx][0]

        # Apply the transformation (if any) to convert the image to a tensor
        if self.transform:
            img = self.transform(img)  # Transformations including ToTensor() are applied here

        # Create a random mask
        mask = self.create_random_mask(img.size())

        # Apply the mask to the image
        masked_img = img * mask

        return masked_img, img, mask

    def create_random_mask(self, size):
        """Create a random mask that blocks a part of the image."""
        mask = torch.ones(size)
        x = random.randint(0, size[1] // 2)
        y = random.randint(0, size[2] // 2)
        w = random.randint(size[1] // 4, size[1] // 2)
        h = random.randint(size[2] // 4, size[2] // 2)

        # Zero out part of the mask
        mask[:, x:x+w, y:y+h] = 0
        return mask

# Example of using the CelebADataset with transformations
transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize images to 128x128
    transforms.ToTensor(),          # Convert the image to a tensor
])

# Create the dataset
dataset = CelebADataset(root_dir='path_to_celeba_dataset', transform=transform)

import torch
import matplotlib.pyplot as plt

def show_inpainting_results(model, dataloader):
    model.eval()  # Set the model to evaluation mode

    with torch.no_grad():  # No need to compute gradients
        for i, (masked_imgs, original_imgs, masks) in enumerate(dataloader):
            # Move data to the appropriate device (GPU/CPU)
            masked_imgs, original_imgs = masked_imgs.to(device), original_imgs.to(device)

            # Get the model's output (inpainted images)
            outputs = model(masked_imgs)

            # Move tensors to CPU and convert to numpy arrays
            outputs = outputs.cpu().numpy().transpose((0, 2, 3, 1))
            original_imgs = original_imgs.cpu().numpy().transpose((0, 2, 3, 1))
            masked_imgs = masked_imgs.cpu().numpy().transpose((0, 2, 3, 1))

            # Unnormalize the images (assuming they were normalized to [-1, 1])
            outputs = (outputs * 0.5) + 0.5  # Scale back to [0, 1]
            original_imgs = (original_imgs * 0.5) + 0.5  # Scale back to [0, 1]
            masked_imgs = (masked_imgs * 0.5) + 0.5  # Scale back to [0, 1]

            # Plot original, masked, and inpainted images
            plt.figure(figsize=(10, 5))
            for j in range(4):  # Show the first 4 images
                plt.subplot(3, 4, j + 1)
                plt.imshow(original_imgs[j])
                plt.axis('off')
                plt.title("Original")

                plt.subplot(3, 4, j + 5)
                plt.imshow(masked_imgs[j])
                plt.axis('off')
                plt.title("Masked")

                plt.subplot(3, 4, j + 9)
                plt.imshow(outputs[j])
                plt.axis('off')
                plt.title("Inpainted")

            plt.show()
            break  # Show only one batch of images

# Save the model
torch.save(model.state_dict(), 'context_encoder.pth')

# Load the model
model.load_state_dict(torch.load('context_encoder.pth'))
model.eval()

!pip install gradio

import torch
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import gradio as gr
import random

# Define the model (Ensure it's consistent with your earlier model definition)
class ContextEncoder(nn.Module):
    def __init__(self):
        super(ContextEncoder, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()  # Output is an image
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Load the trained model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ContextEncoder().to(device)
model.load_state_dict(torch.load('context_encoder.pth', map_location=device))
model.eval()

# Define the inference function
def inpaint_image(image):
    # Preprocess the input image
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension

    # Generate a random mask (similar to training)
    mask = torch.ones_like(image)
    x = random.randint(0, image.size(2) // 2)
    y = random.randint(0, image.size(3) // 2)
    w = random.randint(image.size(2) // 4, image.size(2) // 2)
    h = random.randint(image.size(3) // 4, image.size(3) // 2)
    mask[:, :, x:x+w, y:y+h] = 0

    # Mask the image
    masked_image = image * mask

    # Run the model to inpaint the image
    with torch.no_grad():
        output = model(masked_image).cpu().squeeze(0).numpy().transpose(1, 2, 0)

    # Denormalize the output
    output = (output * 0.5 + 0.5) * 255.0
    output = np.clip(output, 0, 255).astype(np.uint8)

    # Convert to PIL Image and return
    return Image.fromarray(output)

# Create Gradio interface
gr_interface = gr.Interface(
    fn=inpaint_image,  # Function to call for inpainting
    inputs=gr.Image(type="pil"),  # Input as an image
    outputs="image",  # Output as an image
    title="Image Inpainting with Context Encoder",
    description="Upload an image, and the model will fill in the missing parts."
)

# Launch the app
gr_interface.launch(share=True)